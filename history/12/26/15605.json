[{"e_id":"15605","title":"中国百度获AI语言技术王冠","content":"\n    2019年12月26日(农历2019年12月1日)，中国百度获AI语言技术王冠 领先谷歌和微软。　　中国科技巨头百度在一项旨在测试机器能够理解人类语言的人工智能竞赛中已经超越了谷歌和微软。&#13;\n　　正如《麻省理工学院技术评论》2019年12月26日所指出的那样。中国公司百度建立的计算机模型领先于其它所有通用语言理解模型。本月初百度公司在一场持续的人工智能竞争中悄然击败了微软和谷歌，但我们国内关于此次竞赛的报道似乎并不多。&#13;\n&#13;\n　　事实上百度公司不仅仅是一家搜索网站，它还是中国人工智能技术的领先者。这次百度参与的竞争是通用语言理解评估，英文简称为GLUE。百度公司利用Ernie模型成为在GLUE测试中首只超过90分的团队，在由美国科技公司和大学占主导的排行榜中名列榜首。&#13;\n&#13;\n　　这一优异测试结果也使其成为GLUE基准测试中超过平均得分87.1的仅有的10个AI系统之一。微软的D365 AI团队和谷歌的T5团队分列第二三名。百度公司的算法最初设计是用于学习理解中文的，现在它却成为了理解英文语义最好的算法。&#13;\n　　GLUE是一个被普遍接受的基准，用于评估人工智能系统理解人类语言。它由九种不同的测试组成，比如在句子中挑选人名和组织名，以及当有多个潜在先行词时，找出像“It”这样的代词指的是什么。因此一个在GLUE上得分很高的语言模型可以胜任处理不同的阅读理解任务。满分为100分，平均分是87分。ERNIE是百度创建的知识增强的语义表达模型，而对手谷歌则有一个名为BERT的预训练模型。&#13;\n　　GLUE在公开排行榜上的排名在不断变化，虽然还有另外一支团队很可能很快会超越百度。但是百度的成就让人值得注意的是，它展示了AI研究如何从众多贡献者中受益。百度的研究人员不得不针对中文开发一种专门技术来构建ERNIE知识增强语义表达模型。然而让研究人员欣喜的是，该技术也能使人工智能更好地理解英语。&#13;\n&#13;\n　　在谷歌的BERT预训练模型于2018年末创建之前，自然语言模型并不是那么好。之前的自然语言模型擅长预测句子中的下一个单词，因此非常适用于自动完成功能。只是即使是一小段文字，它们也无法训练具有任何思路。这是因为它们不理解含义，例如“它”一词可能指的是什么。先前的模型学会了预测和解释单词的含义，可以仅通过考虑单词之前或之后出现的上下文来理解单词含义，但是它不能同时考虑两者。换句话说它是单向工作的。&#13;\n　　相比之下BERT模型有所改进，BERT模型可以同时考虑单词前后的上下文，使其双向。它使用一种称为“遮罩”的技术来执行此操作。在给定的文本段落中，BERT随机隐藏15%的单词，然后尝试从其余单词中进行预测。这使得它可以做出更准确的预测，因为它可以有两倍的线索可以利用。例如，在“男人去___购买牛奶”一句中，句子的开头和结尾都提示了缺失的单词。 ___是您可以去的地方，也是可以购买牛奶的地方。&#13;\n&#13;\n　　使用遮罩技术是对自然语言任务进行重大改进背后的核心创新之一，并且是诸如OpenAI GPT-2之类的模型能够在不偏离中心主题的情况下写出极具说服力的散文的部分原因。&#13;\n　　从英文到中文再回到英文。当百度研究人员开始开发自己的语言模型时，他们希望以遮罩技术为基础，但是他们意识到他们需要进行调整以适应中文。&#13;\n　　在英语中，单词充当语义单位，这意味着完全脱离上下文的单词仍然包含语义。然而中文汉字却不一样了。虽然某些汉字确实具有内在含义，例如火，水或木。但还有许多汉字只有与其他汉字组合在一起才可以更明确意思。例如汉字灵可以既表示聪明也可以表示灵魂。专有名词中的汉字，例如波士顿或美国，一旦分开讲就不是同一件事了。&#13;\n&#13;\n　　因此研究人员对ERNIE进行了一种新版本的遮罩技术训练，这种遮罩技术可以隐藏多个字符串而不是单个字符串。他们还训练它区分有意义的和随机的字符串，这样它就可以相应地做出正确的字符组合。因此ERNIE更好地掌握了汉字是如何编码信息的，也更准确地预测了缺失的部分。事实证明，这对于翻译和从文本文档中检索信息等应用非常有用。&#13;\n　　研究人员很快发现这种方法实际上对英语也更好。尽管英语不如中文会出现频繁的组合表达意义，但英语具有类似的单词字符串，这些单个单词表示的含义与它们组合在一起截然不同。像“哈利·波特”这样的专有名词和像“切下旧木块”这样的表达就不能通过将它们分离成单独的单词来进行有意义的解析。&#13;\n　　最新版本的ERNIE还使用了其他一些培训技术。比如它能考虑句子的顺序和它们之间的间隔距离，来理解一个段落的逻辑发展。然而最重要的是，它使用了一种称为持续训练的方法，这种方法可以让它在不忘记以前学到的东西的情况下，对新数据和新任务进行训练。这使得它能够越来越好地在尽可能少的人为干扰下执行范围广泛的任务。&#13;\n&#13;\n　　百度正积极利用ERNIE模型为用户提供更多适用的搜索结果，删除新闻源中的重复报道，提高人工智能助手小度准确响应请求的能力。&#13;\n　　他们还将在在一篇论文中详细描述ERNIE的最新架构，该论文将在明年的人工智能发展协会会议上发表。就像百度团队的创新建立在谷歌的BERT模型上一样，研究人员希望其他团队也能从他们研发改进ERNIE的模型中受益。&#13;\n　　百度搜索的首席架构师田浩说：“当我们最初开始这项工作时，就特别考虑了汉语的某些特点，但我们很快发现，它的适用范围远不止这些。”","picNo":"0","picUrl":[]}]